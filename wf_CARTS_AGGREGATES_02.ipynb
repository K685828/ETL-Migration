{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39748f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARTS Aggregates Workflow - PySpark Implementation\n",
    "\n",
    "This notebook converts the Informatica PowerCenter workflow `wf_CARTS_AGGREGATES_02.XML` to PySpark implementation.\n",
    "\n",
    "## Original Workflow Overview\n",
    "- **Workflow Name**: wf_CARTS_AGGREGATES_02\n",
    "- **Description**: CARTS (Commissary Automated Report Transaction System) data aggregation workflow\n",
    "- **Schedule**: Runs every 6 hours starting at 15:00 on 3/10/2025\n",
    "- **Server**: INT_SVC_PROD (Domain_PROD)\n",
    "\n",
    "## Workflow Tasks Sequence\n",
    "The original workflow executes the following tasks in dependency order:\n",
    "\n",
    "1. **Data Preparation**: `cmd_aggts_prep_carts` - Prepares CARTS data for aggregation\n",
    "2. **Dashboard Processing**: \n",
    "   - `cmd_storeops_dshbrd_mnthly` - Monthly store operations dashboard\n",
    "   - `cmd_storeops_dshbrd_fytd` - Fiscal year-to-date store operations dashboard\n",
    "3. **Department Sales Aggregations**:\n",
    "   - `cmd_carts_dept_daily_sales_STR` - Store level department daily sales\n",
    "   - `cmd_carts_dept_daily_sales_MNR` - Minor level department daily sales  \n",
    "   - `cmd_carts_dept_daily_sales_MJR` - Major level department daily sales\n",
    "   - `cmd_carts_dept_daily_sales_MBU` - Major business unit level department daily sales\n",
    "4. **Customer Count Aggregations**:\n",
    "   - `cmd_carts_customer_count_STR` - Store level customer counts\n",
    "   - `cmd_carts_customer_count_MNR` - Minor level customer counts\n",
    "   - `cmd_carts_customer_count_MJR` - Major level customer counts\n",
    "   - `cmd_carts_customer_count_MBU` - Major business unit level customer counts\n",
    "5. **Activity Aggregations**:\n",
    "   - `cmd_carts_daily_terminal_activity_aggt` - Daily terminal activity aggregates\n",
    "   - `cmd_carts_daily_item_sales_aggt` - Daily item sales aggregates\n",
    "6. **Store Statistics**: `cmd_carts_store_daily_stats_incremental` - Incremental store daily statistics\n",
    "7. **Validation**: `cmd_sales_validation_carts` - Sales data validation\n",
    "\n",
    "**Error Handling**: All tasks link to `email_failure` task on failure, which sends notifications to edw_infa_etl@deca.mil and service.desk@deca.mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e22fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('carts_aggregates_workflow.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d62bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "def create_spark_session(app_name: str = \"CARTS_Aggregates_Workflow\") -> SparkSession:\n",
    "    \"\"\"\n",
    "    Create and configure Spark session for CARTS aggregates processing\n",
    "    \"\"\"\n",
    "    conf = SparkConf().setAppName(app_name)\n",
    "    \n",
    "    # Configure Spark for EDW environment\n",
    "    conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    \n",
    "    # Memory and executor configuration for EDW workloads\n",
    "    conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    conf.set(\"spark.executor.cores\", \"4\")\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    \n",
    "    # Oracle database configuration (assuming Oracle backend)\n",
    "    conf.set(\"spark.jars\", \"/opt/spark/jars/ojdbc8.jar\")  # Update path as needed\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    # Set log level\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    logger.info(f\"Spark session created: {app_name}\")\n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = create_spark_session()\n",
    "print(f\"Spark session initialized: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a760946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Constants\n",
    "class CARTSConfig:\n",
    "    \"\"\"Configuration class for CARTS aggregates workflow\"\"\"\n",
    "    \n",
    "    # Database connection parameters\n",
    "    DB_URL = \"jdbc:oracle:thin:@//your-oracle-host:1521/service_name\"  # Update with actual connection\n",
    "    DB_USER = \"edw_user\"  # Update with actual username\n",
    "    DB_PASSWORD = \"your_password\"  # Update with actual password or use secret management\n",
    "    \n",
    "    # Table names\n",
    "    CARTS_SALES_FACT = \"edw.carts_sales_fact\"\n",
    "    CARTS_CUSTOMER_DIM = \"edw.carts_customer_dim\"\n",
    "    CARTS_STORE_DIM = \"edw.carts_store_dim\"\n",
    "    CARTS_PRODUCT_DIM = \"edw.carts_product_dim\"\n",
    "    CARTS_TIME_DIM = \"edw.carts_time_dim\"\n",
    "    \n",
    "    # Output tables\n",
    "    CARTS_DEPT_DAILY_SALES = \"edw.carts_dept_daily_sales_aggt\"\n",
    "    CARTS_CUSTOMER_COUNT = \"edw.carts_customer_count_aggt\"\n",
    "    CARTS_ITEM_SALES = \"edw.carts_daily_item_sales_aggt\"\n",
    "    CARTS_TERMINAL_ACTIVITY = \"edw.carts_daily_terminal_activity_aggt\"\n",
    "    CARTS_STORE_STATS = \"edw.carts_store_daily_stats\"\n",
    "    \n",
    "    # Email configuration\n",
    "    SMTP_SERVER = \"smtp.deca.mil\"  # Update with actual SMTP server\n",
    "    SMTP_PORT = 587\n",
    "    EMAIL_FROM = \"edw_infa_etl@deca.mil\"\n",
    "    EMAIL_TO = [\"edw_infa_etl@deca.mil\", \"service.desk@deca.mil\"]\n",
    "    \n",
    "    # Processing date (default to yesterday)\n",
    "    PROCESS_DATE = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Utility Functions\n",
    "def get_db_connection_properties() -> Dict[str, str]:\n",
    "    \"\"\"Get database connection properties\"\"\"\n",
    "    return {\n",
    "        \"user\": CARTSConfig.DB_USER,\n",
    "        \"password\": CARTSConfig.DB_PASSWORD,\n",
    "        \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n",
    "    }\n",
    "\n",
    "def read_table(spark: SparkSession, table_name: str, condition: str = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read table from database with optional condition\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.jdbc(\n",
    "            url=CARTSConfig.DB_URL,\n",
    "            table=table_name,\n",
    "            properties=get_db_connection_properties()\n",
    "        )\n",
    "        \n",
    "        if condition:\n",
    "            df = df.filter(condition)\n",
    "        \n",
    "        logger.info(f\"Successfully read table: {table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading table {table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_table(df: DataFrame, table_name: str, mode: str = \"overwrite\") -> None:\n",
    "    \"\"\"\n",
    "    Write DataFrame to database table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write.jdbc(\n",
    "            url=CARTSConfig.DB_URL,\n",
    "            table=table_name,\n",
    "            mode=mode,\n",
    "            properties=get_db_connection_properties()\n",
    "        )\n",
    "        logger.info(f\"Successfully wrote to table: {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to table {table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"Configuration and utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Handling and Notification Functions\n",
    "class TaskStatus:\n",
    "    \"\"\"Task status constants\"\"\"\n",
    "    SUCCEEDED = \"SUCCEEDED\"\n",
    "    FAILED = \"FAILED\"\n",
    "    RUNNING = \"RUNNING\"\n",
    "    \n",
    "class WorkflowError(Exception):\n",
    "    \"\"\"Custom exception for workflow errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def send_failure_notification(task_name: str, error_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Send email notification on task failure\n",
    "    Replicates the email_failure task from original workflow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subject = f\"CARTS - {task_name} failed\"\n",
    "        body = f\"\"\"EDW has experienced a PRD load failure of {task_name}.\n",
    "\n",
    "Refer to EDW On-Call Schedule\n",
    "\n",
    "Call EDW primary, secondary, and alternate on-call personnel until contact is made\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "Assign priority 2 Remedy ticket\n",
    "\n",
    "Error Details:\n",
    "{error_message}\n",
    "\n",
    "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "        \n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = CARTSConfig.EMAIL_FROM\n",
    "        msg['To'] = \", \".join(CARTSConfig.EMAIL_TO)\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        \n",
    "        # Note: In production, you would configure SMTP server properly\n",
    "        # For now, we'll just log the notification\n",
    "        logger.error(f\"FAILURE NOTIFICATION - {subject}\")\n",
    "        logger.error(f\"Recipients: {CARTSConfig.EMAIL_TO}\")\n",
    "        logger.error(f\"Message: {body}\")\n",
    "        \n",
    "        print(f\"FAILURE NOTIFICATION SENT: {subject}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send notification: {str(e)}\")\n",
    "\n",
    "def execute_with_error_handling(func, task_name: str, *args, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Execute a function with error handling and notification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting task: {task_name}\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        logger.info(f\"Task {task_name} completed successfully in {duration}\")\n",
    "        \n",
    "        return TaskStatus.SUCCEEDED\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Task {task_name} failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        send_failure_notification(task_name, str(e))\n",
    "        return TaskStatus.FAILED\n",
    "\n",
    "print(\"Error handling and notification functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ba22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Preparation (cmd_aggts_prep_carts)\n",
    "def aggts_prep_carts(spark: SparkSession, process_date: datetime = None) -> None:\n",
    "    \"\"\"\n",
    "    Prepare CARTS data for aggregation processing\n",
    "    Equivalent to: ksh /opt/edw/prod/prep/bin/aggts_prep_carts.sh date-1\n",
    "    \"\"\"\n",
    "    if process_date is None:\n",
    "        process_date = CARTSConfig.PROCESS_DATE\n",
    "    \n",
    "    logger.info(f\"Preparing CARTS data for date: {process_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Read raw CARTS sales data\n",
    "    sales_raw = read_table(\n",
    "        spark, \n",
    "        \"edw.carts_sales_raw\",\n",
    "        f\"sales_date = '{process_date.strftime('%Y-%m-%d')}'\"\n",
    "    )\n",
    "    \n",
    "    # Data quality checks and cleansing\n",
    "    sales_cleaned = sales_raw.filter(\n",
    "        (col(\"sales_amount\").isNotNull()) &\n",
    "        (col(\"sales_amount\") >= 0) &\n",
    "        (col(\"store_id\").isNotNull()) &\n",
    "        (col(\"item_id\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # Add derived columns\n",
    "    sales_prepared = sales_cleaned.withColumn(\n",
    "        \"fiscal_year\", \n",
    "        when(month(col(\"sales_date\")) >= 10, year(col(\"sales_date\")) + 1)\n",
    "        .otherwise(year(col(\"sales_date\")))\n",
    "    ).withColumn(\n",
    "        \"fiscal_quarter\",\n",
    "        when(month(col(\"sales_date\")).isin([10, 11, 12]), 1)\n",
    "        .when(month(col(\"sales_date\")).isin([1, 2, 3]), 2)\n",
    "        .when(month(col(\"sales_date\")).isin([4, 5, 6]), 3)\n",
    "        .otherwise(4)\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\",\n",
    "        current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Write prepared data to staging table\n",
    "    write_table(sales_prepared, \"edw.carts_sales_staging\", \"overwrite\")\n",
    "    \n",
    "    # Log statistics\n",
    "    total_records = sales_prepared.count()\n",
    "    logger.info(f\"Prepared {total_records} CARTS sales records for {process_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(f\"Data preparation completed: {total_records} records processed\")\n",
    "\n",
    "# Execute data preparation task\n",
    "task_status = execute_with_error_handling(\n",
    "    aggts_prep_carts, \n",
    "    \"cmd_aggts_prep_carts\", \n",
    "    spark\n",
    ")\n",
    "print(f\"Data preparation task status: {task_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a27252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Store Operations Dashboard - Monthly (cmd_storeops_dshbrd_mnthly)\n",
    "def storeops_dashboard_monthly(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate monthly store operations dashboard metrics\n",
    "    Equivalent to: ksh /opt/edw/powercenter/server/nm/Scripts/storeops_dshbrd_mnthly.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating monthly store operations dashboard\")\n",
    "    \n",
    "    # Read sales data for current month\n",
    "    current_month = datetime.now().replace(day=1)\n",
    "    previous_month = (current_month - timedelta(days=1)).replace(day=1)\n",
    "    \n",
    "    sales_data = read_table(\n",
    "        spark,\n",
    "        \"edw.carts_sales_staging\",\n",
    "        f\"sales_date >= '{previous_month.strftime('%Y-%m-%d')}' AND sales_date < '{current_month.strftime('%Y-%m-%d')}'\"\n",
    "    )\n",
    "    \n",
    "    # Join with store dimension\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Calculate monthly metrics by store\n",
    "    monthly_metrics = sales_data.join(store_dim, \"store_id\").groupBy(\n",
    "        \"store_id\", \"store_name\", \"region\", \"district\"\n",
    "    ).agg(\n",
    "        sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"total_quantity\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        avg(\"sales_amount\").alias(\"avg_transaction_value\")\n",
    "    ).withColumn(\"report_month\", lit(previous_month.strftime('%Y-%m')))\n",
    "    \n",
    "    # Write to dashboard table\n",
    "    write_table(monthly_metrics, \"edw.storeops_dashboard_monthly\", \"append\")\n",
    "    \n",
    "    record_count = monthly_metrics.count()\n",
    "    logger.info(f\"Generated monthly dashboard for {record_count} stores\")\n",
    "    print(f\"Monthly dashboard completed: {record_count} store records\")\n",
    "\n",
    "# Task 3: Store Operations Dashboard - Fiscal Year to Date (cmd_storeops_dshbrd_fytd)\n",
    "def storeops_dashboard_fytd(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate fiscal year to date store operations dashboard metrics\n",
    "    Equivalent to: ksh /opt/edw/powercenter/server/nm/Scripts/storeops_dshbrd_fytd.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating fiscal year to date store operations dashboard\")\n",
    "    \n",
    "    # Calculate fiscal year start date\n",
    "    current_date = datetime.now()\n",
    "    if current_date.month >= 10:\n",
    "        fy_start = datetime(current_date.year, 10, 1)\n",
    "    else:\n",
    "        fy_start = datetime(current_date.year - 1, 10, 1)\n",
    "    \n",
    "    # Read sales data for fiscal year to date\n",
    "    sales_data = read_table(\n",
    "        spark,\n",
    "        \"edw.carts_sales_staging\",\n",
    "        f\"sales_date >= '{fy_start.strftime('%Y-%m-%d')}'\"\n",
    "    )\n",
    "    \n",
    "    # Join with store dimension\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Calculate FYTD metrics by store\n",
    "    fytd_metrics = sales_data.join(store_dim, \"store_id\").groupBy(\n",
    "        \"store_id\", \"store_name\", \"region\", \"district\", \"fiscal_year\"\n",
    "    ).agg(\n",
    "        sum(\"sales_amount\").alias(\"fytd_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"fytd_quantity\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"fytd_transactions\"),\n",
    "        countDistinct(\"customer_id\").alias(\"fytd_unique_customers\"),\n",
    "        avg(\"sales_amount\").alias(\"fytd_avg_transaction\")\n",
    "    )\n",
    "    \n",
    "    # Write to dashboard table\n",
    "    write_table(fytd_metrics, \"edw.storeops_dashboard_fytd\", \"overwrite\")\n",
    "    \n",
    "    record_count = fytd_metrics.count()\n",
    "    logger.info(f\"Generated FYTD dashboard for {record_count} stores\")\n",
    "    print(f\"FYTD dashboard completed: {record_count} store records\")\n",
    "\n",
    "# Execute dashboard tasks in sequence\n",
    "if task_status == TaskStatus.SUCCEEDED:\n",
    "    monthly_status = execute_with_error_handling(\n",
    "        storeops_dashboard_monthly,\n",
    "        \"cmd_storeops_dshbrd_mnthly\",\n",
    "        spark\n",
    "    )\n",
    "    print(f\"Monthly dashboard task status: {monthly_status}\")\n",
    "    \n",
    "    if monthly_status == TaskStatus.SUCCEEDED:\n",
    "        fytd_status = execute_with_error_handling(\n",
    "            storeops_dashboard_fytd,\n",
    "            \"cmd_storeops_dshbrd_fytd\",\n",
    "            spark\n",
    "        )\n",
    "        print(f\"FYTD dashboard task status: {fytd_status}\")\n",
    "else:\n",
    "    print(\"Skipping dashboard tasks due to data preparation failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd20292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Department Daily Sales Aggregations\n",
    "def carts_dept_daily_sales(spark: SparkSession, aggregation_level: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate daily department sales aggregates at different organizational levels\n",
    "    \n",
    "    Args:\n",
    "        aggregation_level: STR (Store), MNR (Minor), MJR (Major), MBU (Major Business Unit)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating department daily sales aggregates - {aggregation_level} level\")\n",
    "    \n",
    "    # Read sales staging data\n",
    "    sales_data = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    \n",
    "    # Join with product dimension for department information\n",
    "    product_dim = read_table(spark, CARTSConfig.CARTS_PRODUCT_DIM)\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Base aggregation with product and store dimensions\n",
    "    base_agg = sales_data.join(product_dim, \"item_id\") \\\n",
    "                        .join(store_dim, \"store_id\")\n",
    "    \n",
    "    # Define grouping columns based on aggregation level\n",
    "    if aggregation_level == \"STR\":  # Store level\n",
    "        group_cols = [\"store_id\", \"store_name\", \"dept_id\", \"dept_name\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MNR\":  # Minor level (Department grouping)\n",
    "        group_cols = [\"minor_dept_id\", \"minor_dept_name\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MJR\":  # Major level (Major department grouping)\n",
    "        group_cols = [\"major_dept_id\", \"major_dept_name\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MBU\":  # Major Business Unit level\n",
    "        group_cols = [\"business_unit_id\", \"business_unit_name\", \"sales_date\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid aggregation level: {aggregation_level}\")\n",
    "    \n",
    "    # Perform aggregation\n",
    "    dept_sales_agg = base_agg.groupBy(*group_cols).agg(\n",
    "        sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"cost_amount\").alias(\"total_cost\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        avg(\"sales_amount\").alias(\"avg_sales_per_transaction\"),\n",
    "        (sum(\"sales_amount\") - sum(\"cost_amount\")).alias(\"gross_margin\")\n",
    "    ).withColumn(\n",
    "        \"aggregation_level\", lit(aggregation_level)\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Write to department sales aggregate table\n",
    "    table_name = f\"edw.carts_dept_daily_sales_{aggregation_level.lower()}\"\n",
    "    write_table(dept_sales_agg, table_name, \"overwrite\")\n",
    "    \n",
    "    record_count = dept_sales_agg.count()\n",
    "    logger.info(f\"Generated {record_count} department sales records for {aggregation_level} level\")\n",
    "    print(f\"Department sales {aggregation_level} completed: {record_count} records\")\n",
    "\n",
    "# Execute department sales tasks in dependency order\n",
    "if 'fytd_status' in locals() and fytd_status == TaskStatus.SUCCEEDED:\n",
    "    # STR -> MNR -> MJR -> MBU (based on workflow dependencies)\n",
    "    \n",
    "    str_status = execute_with_error_handling(\n",
    "        carts_dept_daily_sales,\n",
    "        \"cmd_carts_dept_daily_sales_STR\",\n",
    "        spark, \"STR\"\n",
    "    )\n",
    "    print(f\"Department sales STR status: {str_status}\")\n",
    "    \n",
    "    if str_status == TaskStatus.SUCCEEDED:\n",
    "        mnr_status = execute_with_error_handling(\n",
    "            carts_dept_daily_sales,\n",
    "            \"cmd_carts_dept_daily_sales_MNR\",\n",
    "            spark, \"MNR\"\n",
    "        )\n",
    "        print(f\"Department sales MNR status: {mnr_status}\")\n",
    "        \n",
    "        if mnr_status == TaskStatus.SUCCEEDED:\n",
    "            mjr_status = execute_with_error_handling(\n",
    "                carts_dept_daily_sales,\n",
    "                \"cmd_carts_dept_daily_sales_MJR\",\n",
    "                spark, \"MJR\"\n",
    "            )\n",
    "            print(f\"Department sales MJR status: {mjr_status}\")\n",
    "            \n",
    "            if mjr_status == TaskStatus.SUCCEEDED:\n",
    "                mbu_status = execute_with_error_handling(\n",
    "                    carts_dept_daily_sales,\n",
    "                    \"cmd_carts_dept_daily_sales_MBU\",\n",
    "                    spark, \"MBU\"\n",
    "                )\n",
    "                print(f\"Department sales MBU status: {mbu_status}\")\n",
    "else:\n",
    "    print(\"Skipping department sales tasks due to dashboard task failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Customer Count Aggregations\n",
    "def carts_customer_count(spark: SparkSession, aggregation_level: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate customer count aggregates at different organizational levels\n",
    "    \n",
    "    Args:\n",
    "        aggregation_level: STR (Store), MNR (Minor), MJR (Major), MBU (Major Business Unit)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating customer count aggregates - {aggregation_level} level\")\n",
    "    \n",
    "    # Read sales staging data\n",
    "    sales_data = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    \n",
    "    # Join with dimensions\n",
    "    customer_dim = read_table(spark, CARTSConfig.CARTS_CUSTOMER_DIM)\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Base data with dimensions\n",
    "    base_data = sales_data.join(customer_dim, \"customer_id\") \\\n",
    "                         .join(store_dim, \"store_id\")\n",
    "    \n",
    "    # Define grouping columns based on aggregation level\n",
    "    if aggregation_level == \"STR\":  # Store level\n",
    "        group_cols = [\"store_id\", \"store_name\", \"region\", \"district\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MNR\":  # Minor level\n",
    "        group_cols = [\"region\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MJR\":  # Major level\n",
    "        group_cols = [\"district\", \"sales_date\"]\n",
    "    elif aggregation_level == \"MBU\":  # Major Business Unit level\n",
    "        group_cols = [\"business_unit_id\", \"business_unit_name\", \"sales_date\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid aggregation level: {aggregation_level}\")\n",
    "    \n",
    "    # Customer count aggregations\n",
    "    customer_counts = base_data.groupBy(*group_cols).agg(\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"total_transactions\"),\n",
    "        count(\"customer_id\").alias(\"total_customer_visits\"),\n",
    "        sum(\"sales_amount\").alias(\"total_sales_amount\"),\n",
    "        avg(\"sales_amount\").alias(\"avg_sales_per_visit\")\n",
    "    ).withColumn(\n",
    "        \"customers_per_transaction\", \n",
    "        col(\"total_customer_visits\") / col(\"total_transactions\")\n",
    "    ).withColumn(\n",
    "        \"sales_per_customer\",\n",
    "        col(\"total_sales_amount\") / col(\"unique_customers\")\n",
    "    ).withColumn(\n",
    "        \"aggregation_level\", lit(aggregation_level)\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Add customer segmentation metrics\n",
    "    customer_segments = base_data.withColumn(\n",
    "        \"customer_segment\",\n",
    "        when(col(\"sales_amount\") >= 100, \"High Value\")\n",
    "        .when(col(\"sales_amount\") >= 50, \"Medium Value\")\n",
    "        .otherwise(\"Low Value\")\n",
    "    ).groupBy(*group_cols, \"customer_segment\").agg(\n",
    "        countDistinct(\"customer_id\").alias(\"segment_customer_count\")\n",
    "    )\n",
    "    \n",
    "    # Pivot customer segments\n",
    "    customer_segments_pivot = customer_segments.groupBy(*group_cols).pivot(\"customer_segment\").sum(\"segment_customer_count\")\n",
    "    \n",
    "    # Join main aggregation with customer segments\n",
    "    final_customer_counts = customer_counts.join(\n",
    "        customer_segments_pivot,\n",
    "        group_cols,\n",
    "        \"left\"\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Write to customer count aggregate table\n",
    "    table_name = f\"edw.carts_customer_count_{aggregation_level.lower()}\"\n",
    "    write_table(final_customer_counts, table_name, \"overwrite\")\n",
    "    \n",
    "    record_count = final_customer_counts.count()\n",
    "    logger.info(f\"Generated {record_count} customer count records for {aggregation_level} level\")\n",
    "    print(f\"Customer count {aggregation_level} completed: {record_count} records\")\n",
    "\n",
    "# Execute customer count tasks in dependency order\n",
    "if 'mbu_status' in locals() and mbu_status == TaskStatus.SUCCEEDED:\n",
    "    # STR -> MNR -> MJR -> MBU (based on workflow dependencies)\n",
    "    \n",
    "    str_customer_status = execute_with_error_handling(\n",
    "        carts_customer_count,\n",
    "        \"cmd_carts_customer_count_STR\",\n",
    "        spark, \"STR\"\n",
    "    )\n",
    "    print(f\"Customer count STR status: {str_customer_status}\")\n",
    "    \n",
    "    if str_customer_status == TaskStatus.SUCCEEDED:\n",
    "        mnr_customer_status = execute_with_error_handling(\n",
    "            carts_customer_count,\n",
    "            \"cmd_carts_customer_count_MNR\",\n",
    "            spark, \"MNR\"\n",
    "        )\n",
    "        print(f\"Customer count MNR status: {mnr_customer_status}\")\n",
    "        \n",
    "        if mnr_customer_status == TaskStatus.SUCCEEDED:\n",
    "            mjr_customer_status = execute_with_error_handling(\n",
    "                carts_customer_count,\n",
    "                \"cmd_carts_customer_count_MJR\",\n",
    "                spark, \"MJR\"\n",
    "            )\n",
    "            print(f\"Customer count MJR status: {mjr_customer_status}\")\n",
    "            \n",
    "            if mjr_customer_status == TaskStatus.SUCCEEDED:\n",
    "                mbu_customer_status = execute_with_error_handling(\n",
    "                    carts_customer_count,\n",
    "                    \"cmd_carts_customer_count_MBU\",\n",
    "                    spark, \"MBU\"\n",
    "                )\n",
    "                print(f\"Customer count MBU status: {mbu_customer_status}\")\n",
    "else:\n",
    "    print(\"Skipping customer count tasks due to department sales task failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Daily Terminal Activity Aggregates\n",
    "def carts_daily_terminal_activity_aggt(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate daily terminal activity aggregates\n",
    "    Equivalent to: ksh /opt/edw/prod/sales_aggts/bin/carts_daily_terminal_activity_aggt.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating daily terminal activity aggregates\")\n",
    "    \n",
    "    # Read sales staging data with terminal information\n",
    "    sales_data = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Terminal activity aggregation\n",
    "    terminal_activity = sales_data.join(store_dim, \"store_id\").groupBy(\n",
    "        \"store_id\", \"store_name\", \"terminal_id\", \"sales_date\"\n",
    "    ).agg(\n",
    "        sum(\"sales_amount\").alias(\"terminal_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"terminal_quantity\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"terminal_transactions\"),\n",
    "        countDistinct(\"customer_id\").alias(\"terminal_customers\"),\n",
    "        min(\"transaction_timestamp\").alias(\"first_transaction_time\"),\n",
    "        max(\"transaction_timestamp\").alias(\"last_transaction_time\"),\n",
    "        avg(\"transaction_duration_seconds\").alias(\"avg_transaction_duration\")\n",
    "    ).withColumn(\n",
    "        \"operating_hours\",\n",
    "        (unix_timestamp(\"last_transaction_time\") - unix_timestamp(\"first_transaction_time\")) / 3600\n",
    "    ).withColumn(\n",
    "        \"transactions_per_hour\",\n",
    "        col(\"terminal_transactions\") / col(\"operating_hours\")\n",
    "    ).withColumn(\n",
    "        \"sales_per_hour\",\n",
    "        col(\"terminal_sales\") / col(\"operating_hours\")\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Write to terminal activity table\n",
    "    write_table(terminal_activity, CARTSConfig.CARTS_TERMINAL_ACTIVITY, \"overwrite\")\n",
    "    \n",
    "    record_count = terminal_activity.count()\n",
    "    logger.info(f\"Generated {record_count} terminal activity records\")\n",
    "    print(f\"Terminal activity aggregation completed: {record_count} records\")\n",
    "\n",
    "# Task 7: Daily Item Sales Aggregates\n",
    "def carts_daily_item_sales_aggt(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate daily item sales aggregates\n",
    "    Equivalent to: ksh /opt/edw/prod/sales_aggts/bin/carts_daily_item_sales_aggt.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating daily item sales aggregates\")\n",
    "    \n",
    "    # Read sales staging data\n",
    "    sales_data = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    product_dim = read_table(spark, CARTSConfig.CARTS_PRODUCT_DIM)\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Item sales aggregation\n",
    "    item_sales = sales_data.join(product_dim, \"item_id\") \\\n",
    "                          .join(store_dim, \"store_id\") \\\n",
    "                          .groupBy(\n",
    "        \"item_id\", \"item_name\", \"item_category\", \"item_subcategory\",\n",
    "        \"store_id\", \"store_name\", \"sales_date\"\n",
    "    ).agg(\n",
    "        sum(\"sales_amount\").alias(\"item_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"item_quantity_sold\"),\n",
    "        sum(\"cost_amount\").alias(\"item_cost\"),\n",
    "        countDistinct(\"transaction_id\").alias(\"transactions_with_item\"),\n",
    "        countDistinct(\"customer_id\").alias(\"customers_bought_item\"),\n",
    "        avg(\"sales_amount\").alias(\"avg_item_price\"),\n",
    "        (sum(\"sales_amount\") - sum(\"cost_amount\")).alias(\"item_gross_margin\")\n",
    "    ).withColumn(\n",
    "        \"margin_percentage\",\n",
    "        (col(\"item_gross_margin\") / col(\"item_sales\")) * 100\n",
    "    ).withColumn(\n",
    "        \"sales_velocity\",\n",
    "        col(\"item_quantity_sold\") / col(\"transactions_with_item\")\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Add ranking by sales performance\n",
    "    item_sales_ranked = item_sales.withColumn(\n",
    "        \"sales_rank\",\n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"store_id\", \"sales_date\")\n",
    "                  .orderBy(desc(\"item_sales\"))\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"quantity_rank\",\n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"store_id\", \"sales_date\")\n",
    "                  .orderBy(desc(\"item_quantity_sold\"))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Write to item sales aggregate table\n",
    "    write_table(item_sales_ranked, CARTSConfig.CARTS_ITEM_SALES, \"overwrite\")\n",
    "    \n",
    "    record_count = item_sales_ranked.count()\n",
    "    logger.info(f\"Generated {record_count} item sales records\")\n",
    "    print(f\"Item sales aggregation completed: {record_count} records\")\n",
    "\n",
    "# Execute terminal activity and item sales tasks\n",
    "if 'mbu_customer_status' in locals() and mbu_customer_status == TaskStatus.SUCCEEDED:\n",
    "    \n",
    "    terminal_status = execute_with_error_handling(\n",
    "        carts_daily_terminal_activity_aggt,\n",
    "        \"cmd_carts_daily_terminal_activity_aggt\",\n",
    "        spark\n",
    "    )\n",
    "    print(f\"Terminal activity task status: {terminal_status}\")\n",
    "    \n",
    "    if terminal_status == TaskStatus.SUCCEEDED:\n",
    "        item_sales_status = execute_with_error_handling(\n",
    "            carts_daily_item_sales_aggt,\n",
    "            \"cmd_carts_daily_item_sales_aggt\",\n",
    "            spark\n",
    "        )\n",
    "        print(f\"Item sales task status: {item_sales_status}\")\n",
    "else:\n",
    "    print(\"Skipping terminal activity and item sales tasks due to customer count task failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ce12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Store Daily Statistics Incremental\n",
    "def carts_store_daily_stats_incremental(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate incremental store daily statistics\n",
    "    Equivalent to: ksh /opt/edw/prod/sales_aggts/bin/carts_store_daily_stats_incremental.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating incremental store daily statistics\")\n",
    "    \n",
    "    # Read required data\n",
    "    sales_data = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    store_dim = read_table(spark, CARTSConfig.CARTS_STORE_DIM)\n",
    "    \n",
    "    # Calculate comprehensive store statistics\n",
    "    store_stats = sales_data.join(store_dim, \"store_id\").groupBy(\n",
    "        \"store_id\", \"store_name\", \"region\", \"district\", \"store_type\", \"sales_date\"\n",
    "    ).agg(\n",
    "        # Sales metrics\n",
    "        sum(\"sales_amount\").alias(\"daily_sales\"),\n",
    "        sum(\"sales_quantity\").alias(\"daily_quantity\"),\n",
    "        sum(\"cost_amount\").alias(\"daily_cost\"),\n",
    "        \n",
    "        # Transaction metrics\n",
    "        countDistinct(\"transaction_id\").alias(\"daily_transactions\"),\n",
    "        countDistinct(\"customer_id\").alias(\"daily_customers\"),\n",
    "        countDistinct(\"item_id\").alias(\"items_sold\"),\n",
    "        \n",
    "        # Performance metrics\n",
    "        avg(\"sales_amount\").alias(\"avg_transaction_value\"),\n",
    "        min(\"sales_amount\").alias(\"min_transaction_value\"),\n",
    "        max(\"sales_amount\").alias(\"max_transaction_value\"),\n",
    "        \n",
    "        # Operational metrics\n",
    "        countDistinct(\"terminal_id\").alias(\"active_terminals\"),\n",
    "        min(\"transaction_timestamp\").alias(\"first_sale_time\"),\n",
    "        max(\"transaction_timestamp\").alias(\"last_sale_time\")\n",
    "    ).withColumn(\n",
    "        # Calculated metrics\n",
    "        \"gross_margin\", col(\"daily_sales\") - col(\"daily_cost\")\n",
    "    ).withColumn(\n",
    "        \"margin_percentage\", (col(\"gross_margin\") / col(\"daily_sales\")) * 100\n",
    "    ).withColumn(\n",
    "        \"sales_per_customer\", col(\"daily_sales\") / col(\"daily_customers\")\n",
    "    ).withColumn(\n",
    "        \"transactions_per_customer\", col(\"daily_transactions\") / col(\"daily_customers\")\n",
    "    ).withColumn(\n",
    "        \"items_per_transaction\", col(\"items_sold\") / col(\"daily_transactions\")\n",
    "    ).withColumn(\n",
    "        \"operating_hours\",\n",
    "        (unix_timestamp(\"last_sale_time\") - unix_timestamp(\"first_sale_time\")) / 3600\n",
    "    ).withColumn(\n",
    "        \"sales_per_hour\", col(\"daily_sales\") / col(\"operating_hours\")\n",
    "    ).withColumn(\n",
    "        \"process_timestamp\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    # Add comparative metrics (vs previous day, week, month)\n",
    "    previous_day_stats = read_table(\n",
    "        spark, \n",
    "        CARTSConfig.CARTS_STORE_STATS,\n",
    "        f\"sales_date = '{(CARTSConfig.PROCESS_DATE - timedelta(days=1)).strftime('%Y-%m-%d')}'\"\n",
    "    ).select(\n",
    "        col(\"store_id\").alias(\"prev_store_id\"),\n",
    "        col(\"daily_sales\").alias(\"prev_day_sales\"),\n",
    "        col(\"daily_transactions\").alias(\"prev_day_transactions\")\n",
    "    )\n",
    "    \n",
    "    # Join with previous day data for comparison\n",
    "    store_stats_with_comparison = store_stats.join(\n",
    "        previous_day_stats,\n",
    "        store_stats.store_id == previous_day_stats.prev_store_id,\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"sales_growth_pct\",\n",
    "        ((col(\"daily_sales\") - col(\"prev_day_sales\")) / col(\"prev_day_sales\")) * 100\n",
    "    ).withColumn(\n",
    "        \"transaction_growth_pct\",\n",
    "        ((col(\"daily_transactions\") - col(\"prev_day_transactions\")) / col(\"prev_day_transactions\")) * 100\n",
    "    ).drop(\"prev_store_id\", \"prev_day_sales\", \"prev_day_transactions\")\n",
    "    \n",
    "    # Write to store statistics table\n",
    "    write_table(store_stats_with_comparison, CARTSConfig.CARTS_STORE_STATS, \"append\")\n",
    "    \n",
    "    record_count = store_stats_with_comparison.count()\n",
    "    logger.info(f\"Generated {record_count} store statistics records\")\n",
    "    print(f\"Store statistics completed: {record_count} records\")\n",
    "\n",
    "# Task 9: Sales Validation\n",
    "def sales_validation_carts(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Perform sales data validation checks\n",
    "    Equivalent to: ksh /opt/edw/management/etlchecks/scripts/sales_validation_carts.sh\n",
    "    \"\"\"\n",
    "    logger.info(\"Performing CARTS sales data validation\")\n",
    "    \n",
    "    # Read staging and aggregate data for validation\n",
    "    sales_staging = read_table(spark, \"edw.carts_sales_staging\")\n",
    "    store_stats = read_table(spark, CARTSConfig.CARTS_STORE_STATS)\n",
    "    \n",
    "    # Validation checks\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check 1: Total sales amount consistency\n",
    "    staging_total = sales_staging.agg(sum(\"sales_amount\")).collect()[0][0]\n",
    "    stats_total = store_stats.filter(\n",
    "        col(\"sales_date\") == CARTSConfig.PROCESS_DATE.strftime('%Y-%m-%d')\n",
    "    ).agg(sum(\"daily_sales\")).collect()[0][0]\n",
    "    \n",
    "    sales_variance = abs(staging_total - stats_total) if stats_total else float('inf')\n",
    "    sales_variance_pct = (sales_variance / staging_total) * 100 if staging_total > 0 else 0\n",
    "    \n",
    "    validation_results.append({\n",
    "        \"check_name\": \"Sales Amount Consistency\",\n",
    "        \"staging_value\": staging_total,\n",
    "        \"aggregate_value\": stats_total,\n",
    "        \"variance\": sales_variance,\n",
    "        \"variance_pct\": sales_variance_pct,\n",
    "        \"status\": \"PASS\" if sales_variance_pct < 1.0 else \"FAIL\"\n",
    "    })\n",
    "    \n",
    "    # Check 2: Record count validation\n",
    "    staging_count = sales_staging.count()\n",
    "    expected_min_records = 1000  # Configure based on business rules\n",
    "    \n",
    "    validation_results.append({\n",
    "        \"check_name\": \"Record Count Validation\",\n",
    "        \"staging_value\": staging_count,\n",
    "        \"expected_min\": expected_min_records,\n",
    "        \"status\": \"PASS\" if staging_count >= expected_min_records else \"FAIL\"\n",
    "    })\n",
    "    \n",
    "    # Check 3: Data quality checks\n",
    "    null_sales = sales_staging.filter(col(\"sales_amount\").isNull()).count()\n",
    "    null_customers = sales_staging.filter(col(\"customer_id\").isNull()).count()\n",
    "    negative_sales = sales_staging.filter(col(\"sales_amount\") < 0).count()\n",
    "    \n",
    "    validation_results.append({\n",
    "        \"check_name\": \"Data Quality\",\n",
    "        \"null_sales\": null_sales,\n",
    "        \"null_customers\": null_customers,\n",
    "        \"negative_sales\": negative_sales,\n",
    "        \"status\": \"PASS\" if (null_sales + null_customers + negative_sales) == 0 else \"FAIL\"\n",
    "    })\n",
    "    \n",
    "    # Log validation results\n",
    "    for result in validation_results:\n",
    "        logger.info(f\"Validation Check: {result}\")\n",
    "        if result[\"status\"] == \"FAIL\":\n",
    "            logger.error(f\"VALIDATION FAILURE: {result['check_name']}\")\n",
    "    \n",
    "    # Create validation summary DataFrame\n",
    "    validation_df = spark.createDataFrame([\n",
    "        {\n",
    "            \"validation_date\": CARTSConfig.PROCESS_DATE.strftime('%Y-%m-%d'),\n",
    "            \"total_checks\": len(validation_results),\n",
    "            \"passed_checks\": len([r for r in validation_results if r[\"status\"] == \"PASS\"]),\n",
    "            \"failed_checks\": len([r for r in validation_results if r[\"status\"] == \"FAIL\"]),\n",
    "            \"staging_records\": staging_count,\n",
    "            \"staging_sales_total\": staging_total,\n",
    "            \"process_timestamp\": datetime.now()\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # Write validation results\n",
    "    write_table(validation_df, \"edw.carts_validation_results\", \"append\")\n",
    "    \n",
    "    # Check if any validations failed\n",
    "    failed_checks = [r for r in validation_results if r[\"status\"] == \"FAIL\"]\n",
    "    if failed_checks:\n",
    "        raise WorkflowError(f\"Validation failed: {len(failed_checks)} checks failed\")\n",
    "    \n",
    "    print(f\"Sales validation completed: All {len(validation_results)} checks passed\")\n",
    "\n",
    "# Execute store statistics and validation tasks\n",
    "if 'item_sales_status' in locals() and item_sales_status == TaskStatus.SUCCEEDED:\n",
    "    \n",
    "    store_stats_status = execute_with_error_handling(\n",
    "        carts_store_daily_stats_incremental,\n",
    "        \"cmd_carts_store_daily_stats_incremental\",\n",
    "        spark\n",
    "    )\n",
    "    print(f\"Store statistics task status: {store_stats_status}\")\n",
    "    \n",
    "    if store_stats_status == TaskStatus.SUCCEEDED:\n",
    "        validation_status = execute_with_error_handling(\n",
    "            sales_validation_carts,\n",
    "            \"cmd_sales_validation_carts\",\n",
    "            spark\n",
    "        )\n",
    "        print(f\"Sales validation task status: {validation_status}\")\n",
    "else:\n",
    "    print(\"Skipping store statistics and validation tasks due to item sales task failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff883e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Summary and Cleanup\n",
    "def generate_workflow_summary(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Generate a summary of the entire workflow execution\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating workflow execution summary\")\n",
    "    \n",
    "    # Collect task statuses\n",
    "    task_summary = {\n",
    "        \"workflow_name\": \"wf_CARTS_AGGREGATES_02_PySpark\",\n",
    "        \"execution_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"process_date\": CARTSConfig.PROCESS_DATE.strftime('%Y-%m-%d'),\n",
    "        \"total_tasks\": 9,\n",
    "        \"completed_tasks\": 0,\n",
    "        \"failed_tasks\": 0,\n",
    "        \"task_details\": []\n",
    "    }\n",
    "    \n",
    "    # Check each task status (if variables exist)\n",
    "    task_checks = [\n",
    "        (\"Data Preparation\", 'task_status', \"cmd_aggts_prep_carts\"),\n",
    "        (\"Monthly Dashboard\", 'monthly_status', \"cmd_storeops_dshbrd_mnthly\"),\n",
    "        (\"FYTD Dashboard\", 'fytd_status', \"cmd_storeops_dshbrd_fytd\"),\n",
    "        (\"Dept Sales STR\", 'str_status', \"cmd_carts_dept_daily_sales_STR\"),\n",
    "        (\"Dept Sales MNR\", 'mnr_status', \"cmd_carts_dept_daily_sales_MNR\"),\n",
    "        (\"Dept Sales MJR\", 'mjr_status', \"cmd_carts_dept_daily_sales_MJR\"),\n",
    "        (\"Dept Sales MBU\", 'mbu_status', \"cmd_carts_dept_daily_sales_MBU\"),\n",
    "        (\"Customer Count STR\", 'str_customer_status', \"cmd_carts_customer_count_STR\"),\n",
    "        (\"Customer Count MNR\", 'mnr_customer_status', \"cmd_carts_customer_count_MNR\"),\n",
    "        (\"Customer Count MJR\", 'mjr_customer_status', \"cmd_carts_customer_count_MJR\"),\n",
    "        (\"Customer Count MBU\", 'mbu_customer_status', \"cmd_carts_customer_count_MBU\"),\n",
    "        (\"Terminal Activity\", 'terminal_status', \"cmd_carts_daily_terminal_activity_aggt\"),\n",
    "        (\"Item Sales\", 'item_sales_status', \"cmd_carts_daily_item_sales_aggt\"),\n",
    "        (\"Store Statistics\", 'store_stats_status', \"cmd_carts_store_daily_stats_incremental\"),\n",
    "        (\"Sales Validation\", 'validation_status', \"cmd_sales_validation_carts\")\n",
    "    ]\n",
    "    \n",
    "    for task_name, status_var, original_task in task_checks:\n",
    "        if status_var in globals():\n",
    "            status = globals()[status_var]\n",
    "            task_summary[\"task_details\"].append({\n",
    "                \"task_name\": task_name,\n",
    "                \"original_task\": original_task,\n",
    "                \"status\": status\n",
    "            })\n",
    "            if status == TaskStatus.SUCCEEDED:\n",
    "                task_summary[\"completed_tasks\"] += 1\n",
    "            else:\n",
    "                task_summary[\"failed_tasks\"] += 1\n",
    "    \n",
    "    # Calculate success rate\n",
    "    if task_summary[\"total_tasks\"] > 0:\n",
    "        success_rate = (task_summary[\"completed_tasks\"] / len(task_summary[\"task_details\"])) * 100\n",
    "        task_summary[\"success_rate\"] = success_rate\n",
    "    \n",
    "    # Log summary\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"CARTS AGGREGATES WORKFLOW SUMMARY\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Workflow: {task_summary['workflow_name']}\")\n",
    "    logger.info(f\"Execution Date: {task_summary['execution_date']}\")\n",
    "    logger.info(f\"Process Date: {task_summary['process_date']}\")\n",
    "    logger.info(f\"Tasks Completed: {task_summary['completed_tasks']}\")\n",
    "    logger.info(f\"Tasks Failed: {task_summary['failed_tasks']}\")\n",
    "    if 'success_rate' in task_summary:\n",
    "        logger.info(f\"Success Rate: {task_summary['success_rate']:.1f}%\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    for task in task_summary[\"task_details\"]:\n",
    "        status_symbol = \"✓\" if task[\"status\"] == TaskStatus.SUCCEEDED else \"✗\"\n",
    "        logger.info(f\"{status_symbol} {task['task_name']}: {task['status']}\")\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Create summary DataFrame for reporting\n",
    "    summary_df = spark.createDataFrame([task_summary])\n",
    "    write_table(summary_df, \"edw.carts_workflow_execution_log\", \"append\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"CARTS AGGREGATES WORKFLOW COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Process Date: {task_summary['process_date']}\")\n",
    "    print(f\"Completed Tasks: {task_summary['completed_tasks']}\")\n",
    "    print(f\"Failed Tasks: {task_summary['failed_tasks']}\")\n",
    "    if 'success_rate' in task_summary:\n",
    "        print(f\"Success Rate: {task_summary['success_rate']:.1f}%\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def cleanup_spark_session(spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    Clean up Spark session and resources\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear cache\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "        \n",
    "        logger.info(\"Spark session cleaned up successfully\")\n",
    "        print(\"Spark session cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "# Execute workflow summary\n",
    "generate_workflow_summary(spark)\n",
    "\n",
    "# Note: Uncomment the following line to cleanup Spark session\n",
    "# cleanup_spark_session(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa780e26",
   "metadata": {},
   "source": [
    "## Implementation Notes and Configuration\n",
    "\n",
    "### Prerequisites\n",
    "1. **PySpark Environment**: Ensure PySpark is installed with appropriate version\n",
    "2. **Database Connectivity**: Oracle JDBC driver (`ojdbc8.jar`) must be available in Spark classpath\n",
    "3. **Database Access**: Configure connection parameters in `CARTSConfig` class\n",
    "4. **Email Configuration**: Update SMTP settings for failure notifications\n",
    "\n",
    "### Configuration Updates Required\n",
    "1. **Database Connection**: Update `CARTSConfig.DB_URL`, `DB_USER`, `DB_PASSWORD`\n",
    "2. **Table Names**: Verify and update table names in `CARTSConfig` to match your schema\n",
    "3. **SMTP Settings**: Configure email server details for notifications\n",
    "4. **Spark Configuration**: Adjust memory and executor settings based on your cluster\n",
    "\n",
    "### Scheduling Equivalent\n",
    "The original workflow runs every 6 hours. To replicate this in a production environment:\n",
    "\n",
    "**Option 1: Cron Job**\n",
    "```bash\n",
    "0 */6 * * * /path/to/spark-submit --py-files /path/to/notebook.py\n",
    "```\n",
    "\n",
    "**Option 2: Airflow DAG**\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "dag = DAG(\n",
    "    'carts_aggregates_workflow',\n",
    "    schedule_interval='0 */6 * * *',  # Every 6 hours\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "spark_task = SparkSubmitOperator(\n",
    "    task_id='carts_aggregates',\n",
    "    application='/path/to/carts_aggregates.py',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance Optimizations\n",
    "1. **Partitioning**: Consider partitioning large tables by date\n",
    "2. **Caching**: Cache frequently accessed dimension tables\n",
    "3. **Broadcast Joins**: Use broadcast hints for small dimension tables\n",
    "4. **Parallel Execution**: Leverage Spark's parallel processing capabilities\n",
    "\n",
    "### Monitoring and Alerting\n",
    "1. **Logging**: All tasks log to both file and console\n",
    "2. **Email Notifications**: Automatic failure notifications to operations team\n",
    "3. **Validation Checks**: Built-in data quality and consistency checks\n",
    "4. **Execution Summary**: Comprehensive workflow execution reporting\n",
    "\n",
    "### Error Recovery\n",
    "- Each task is wrapped with error handling\n",
    "- Failed tasks trigger email notifications\n",
    "- Workflow continues where possible (dependent tasks are skipped)\n",
    "- Detailed error logging for troubleshooting\n",
    "\n",
    "### Data Quality Features\n",
    "- Null value checks\n",
    "- Negative value validation\n",
    "- Cross-aggregate consistency verification\n",
    "- Record count validation\n",
    "- Variance analysis between staging and aggregate data\n",
    "\n",
    "This PySpark implementation provides equivalent functionality to the original Informatica PowerCenter workflow while leveraging modern big data processing capabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
